{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43adbb6a-07b0-4493-a596-eec6921730c5",
   "metadata": {},
   "source": [
    "# 头脑风暴\n",
    "\n",
    "一些尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43b5ba2-a1dd-4faa-882e-1a96267193c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a63d740-ca03-425d-9330-bf4d2995c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_BERT_PATH = './data/bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a8f0a-8f19-49ad-86fc-562b87facdfe",
   "metadata": {},
   "source": [
    "## 1. Embedding 的稳定性\n",
    "\n",
    "同一个词做两次 embedding，结果是否相同？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0b3bb4-4d2a-414a-bfbb-22f9c9e732aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./data/bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "tokenizer = BertTokenizer.from_pretrained(EN_BERT_PATH)\n",
    "model = BertModel.from_pretrained(EN_BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087130e9-effa-4774-8769-aefa9fd70612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量计算 embeddings\n",
    "def get_embeddings(corpus):\n",
    "    encoded_inputs = tokenizer(corpus,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "106d7a7a-0a97-4edb-88c7-4dfefa66e9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = get_embeddings('usa')\n",
    "e2 = get_embeddings('usa')\n",
    "\n",
    "all((e1 == e2).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a54008-0f48-4834-8c64-d14e235f619f",
   "metadata": {},
   "source": [
    "## 2. Embedding 差值的意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e19ddb-4101-4a8d-9a4e-1bdb51c1b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'king',\n",
    "    'queen',\n",
    "    'male',\n",
    "    'female',\n",
    "    'man',\n",
    "    'woman',\n",
    "    'blue',\n",
    "    'bad'\n",
    "]\n",
    "embeddings = get_embeddings(corpus)\n",
    "c2e = {c: e for c, e in zip(corpus, embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4239340a-24e1-4d8e-a9f1-9e3849772662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_distances(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.sum((a - b) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e66d5f3-213b-4c9c-9996-3b554eb29e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.46646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_distances(a=c2e['king'],\n",
    "                  b=c2e['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "526be46f-9265-479a-b819-199f9df1f851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.630802"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_distances(a=c2e['male'],\n",
    "                  b=c2e['female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dec22fa-f502-4f52-985e-9f1fb553948e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.933697"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_distances(a=c2e['man'],\n",
    "                  b=c2e['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54ff1fb-8634-44da-b7f8-33fa1ff7ddd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.82113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared_distances(a=c2e['blue'],\n",
    "                  b=c2e['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cd105-31cd-4b6d-bef5-8744ff476fda",
   "metadata": {},
   "source": [
    "## 3. 输出 embedding 的数量\n",
    "\n",
    "考虑一件事，输入三个英文单词，会输出三个 embedding 向量吗？\n",
    "\n",
    "输出 embedding 的数量是由输入单词的数量决定的吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048b5c55-29e2-4260-a312-46004d84e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量计算 last_hidden_state\n",
    "def get_last_hidden_state(corpus):\n",
    "    encoded_inputs = tokenizer(corpus,\n",
    "                               truncation=True,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    return encoded_inputs, last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "346bc807-8155-4d76-8eef-cae9a5b8d332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count of corpus_1: 14\n",
      "word count of corpus_2: 4\n",
      "word count of corpus_3: 8\n"
     ]
    }
   ],
   "source": [
    "corpus_1 = 'The number of embeddings corresponds to the number of tokens generated by the tokenizer.'\n",
    "print('word count of corpus_1:', len(corpus_1.split(' ')))\n",
    "\n",
    "corpus_2 = 'input a single word'\n",
    "print('word count of corpus_2:', len(corpus_2.split(' ')))\n",
    "\n",
    "corpus_3 = 'BERT will still generate an embedding for it'\n",
    "print('word count of corpus_3:', len(corpus_3.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4432dd0b-9efa-4a27-88da-69ed12e03e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token: 22\n",
      "Number of embedding: 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 22]), torch.Size([1, 22, 768]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs, last_hidden_state = get_last_hidden_state(corpus_1)\n",
    "print('Number of token:', encoded_inputs['input_ids'].shape[1])\n",
    "print('Number of embedding:', last_hidden_state.shape[1])\n",
    "\n",
    "encoded_inputs['input_ids'].shape, last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5ad6ac-1764-422a-8d86-53e181234ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token: 6\n",
      "Number of embedding: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]), torch.Size([1, 6, 768]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs, last_hidden_state = get_last_hidden_state(corpus_2)\n",
    "print('Number of token:', encoded_inputs['input_ids'].shape[1])\n",
    "print('Number of embedding:', last_hidden_state.shape[1])\n",
    "\n",
    "encoded_inputs['input_ids'].shape, last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0f1be5-efb7-41aa-a339-8c767cec98dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token: 12\n",
      "Number of embedding: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12]), torch.Size([1, 12, 768]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs, last_hidden_state = get_last_hidden_state(corpus_3)\n",
    "print('Number of token:', encoded_inputs['input_ids'].shape[1])\n",
    "print('Number of embedding:', last_hidden_state.shape[1])\n",
    "\n",
    "encoded_inputs['input_ids'].shape, last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ef61f-6cfa-44bb-9602-d109de16a1ef",
   "metadata": {},
   "source": [
    "与 ChatGPT 的对话：\n",
    "\n",
    "省流版 -> 生成嵌入向量的数量是由 `tokenizer` 决定的。\n",
    "\n",
    "> **Q:** What determines the number of embeddings output by BERT? If I input a word, how many embeddings will it output?\n",
    "> \n",
    "> **ChatGPT:**\n",
    "> \n",
    "> The number of embeddings output by BERT depends on the length of the input and the specific tokenization process applied to it. Here's a detailed explanation:\n",
    "> \n",
    "> 1. **Tokenization**: BERT uses a WordPiece tokenizer. This means that the input text is first split into tokens, which may be words or subwords. For example, the word \"unhappiness\" might be tokenized into [\"un\", \"##happy\", \"##ness\"]. Each of these tokens will then have its own embedding.\n",
    "> \n",
    "> 2. **Input Length**: The number of embeddings corresponds to the number of tokens generated by the tokenizer. If your input is a single word, it will be tokenized into one or more tokens, depending on whether it is present in the tokenizer's vocabulary as a whole or needs to be broken down into subwords.\n",
    "> \n",
    "> 3. **Special Tokens**: For sentences or longer text, BERT also adds special tokens like `[CLS]` at the beginning and `[SEP]` at the end. These special tokens are also considered in the count of embeddings.\n",
    "> \n",
    "> ### Example\n",
    "> \n",
    "> - **Single Word Input**: If you input the word \"unhappiness\":\n",
    ">   - It might be tokenized as [\"un\", \"##happy\", \"##ness\"].\n",
    ">   - BERT will output an embedding for each token, resulting in three embeddings in this case.\n",
    "> \n",
    "> - **Sentence Input**: If you input the sentence \"I am happy\":\n",
    ">   - It might be tokenized as [\"[CLS]\", \"i\", \"am\", \"happy\", \"[SEP]\"].\n",
    ">   - BERT will output five embeddings, one for each token.\n",
    "> \n",
    "> ### Embedding Dimensions\n",
    "> \n",
    "> Regardless of the number of tokens, each token embedding from BERT has a fixed size (typically 768 dimensions for BERT base and 1024 dimensions for BERT large). Therefore, if you input a sentence and it gets tokenized into 10 tokens, you will receive 10 embeddings, each of size 768 (for BERT base).\n",
    "> \n",
    "> ### Summary\n",
    "> \n",
    "> - **Number of embeddings**: Determined by the number of tokens produced by the tokenizer.\n",
    "> - **Embedding size**: Fixed and depends on the BERT model variant (768 for BERT base, 1024 for BERT large).\n",
    "> \n",
    "> If you input a single word, the number of embeddings will be equal to the number of tokens that the word is split into by the tokenizer. For a single word that is not split further, it will output one embedding. If the word is split into subwords, it will output one embedding per subword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1db557-eb6b-4adf-a5c8-29a097152526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
