{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261f50fe-ecba-4a58-894e-e7dc9e11800e",
   "metadata": {},
   "source": [
    "# 寻找最近邻 embedding\n",
    "\n",
    "我的构想是，拿到红楼梦里所有词汇的embedding，然后看哪个词离我们感兴趣的词（比如：林黛玉）最近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b46afac-3c97-4e9d-be9b-7bfd7927c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0v/110wmd1964s9xk3hg_ty7hnh0000gn/T/jieba.cache\n",
      "Loading model cost 0.253 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import jieba\n",
    "\n",
    "jieba.load_userdict('./data/user_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1913cf-d12d-4310-9a6a-23c9482e313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_BERT_PATH = './data/bert-base-chinese'\n",
    "CN_BOOK_PATH = './data/红楼梦.txt'\n",
    "CN_STOP_WORDS = './data/cn_stopwords.txt'\n",
    "MIN_FREQ = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f1d37-581a-4751-b996-2c8a62a92b52",
   "metadata": {},
   "source": [
    "## 1. 分词\n",
    "\n",
    "首先做文本预处理，对《红楼梦》做分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9ac42b-efad-4a8c-899b-42c528ad266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "def load_stop_words(stop_words_path):\n",
    "    with open(stop_words_path, 'r') as f:\n",
    "        stop_words = f.read()\n",
    "    return stop_words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780799a4-6366-4861-9a9b-770161e46ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对《红楼梦》做分词\n",
    "def preprocess(book_path, cn_stop_words_path):\n",
    "    with open(book_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "        # 删除 \\n \\u3000 \\u3000\n",
    "        pattern = re.compile(r'(\\n|\\u3000|\\u3000)', re.IGNORECASE)\n",
    "        text = pattern.sub('', content)\n",
    "        \n",
    "        # 加载中文停用词\n",
    "        cn_stop_words = load_stop_words(cn_stop_words_path)\n",
    "        \n",
    "        # 计算分词\n",
    "        corpus = [w for w in jieba.lcut(text)\n",
    "                  if w not in cn_stop_words and len(w) > 1]\n",
    "\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2782c0eb-f40b-46a0-949f-c808ec3e6696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198712"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = preprocess(book_path=CN_BOOK_PATH,\n",
    "                    cn_stop_words_path=CN_STOP_WORDS)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dabc448-a16f-44bf-bf51-d05d0dbbec55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('宝玉', 3762), ('一个', 1356), ('贾母', 1323), ('凤姐', 1202), ('王夫人', 1020)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词频统计\n",
    "ctr = collections.Counter(corpus)\n",
    "\n",
    "# 高频词\n",
    "ctr.most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a0840f-2656-4ecb-bf13-374e6b80135f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('抄者', 1), ('游戏笔墨', 1), ('陶情适性', 1), ('曾题', 1), ('更进一竿', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 低频词\n",
    "ctr.most_common()[-5:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428979ed-9bc3-46b4-8c2a-1434a75e5f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤出现次数过低的词\n",
    "n_corpus = [k for k, v in ctr.items() if v > MIN_FREQ]\n",
    "len(n_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f5d27f-9dcf-4bd5-afe4-07fad3af26d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['下回分解']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看一下最长的词汇\n",
    "max_length = max([len(e) for e in n_corpus])\n",
    "print('max_length:', max_length)\n",
    "\n",
    "[e for e in n_corpus if len(e) == max_length][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65910596-8b48-4733-b7bb-355539fd2538",
   "metadata": {},
   "source": [
    "## 2. 批量计算 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc7edec0-f498-4729-9b99-5a7245dd8ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./data/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(CN_BERT_PATH)\n",
    "model = BertModel.from_pretrained(CN_BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02b3c0d-df15-49da-8abf-609e80bf1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量计算 embeddings\n",
    "def get_embeddings(corpus):\n",
    "    encoded_inputs = tokenizer(corpus,\n",
    "                               padding='max_length',\n",
    "                               truncation=True,\n",
    "                               return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        # embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8838a760-b8d1-465c-8707-4e39605b32cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_embeddings(corpus=n_corpus[:3])\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc774d2a-82a8-40ce-b5da-5f4b84509441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8313,  0.6034,  0.0404,  ...,  0.2570,  0.2097,  0.1184],\n",
       "        [-0.8494,  0.4742, -0.7889,  ...,  0.5198,  0.0041,  0.0336],\n",
       "        [-0.7053,  0.1953, -0.2742,  ...,  0.4208,  0.0719, -0.1333]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a3930-9f51-42ba-bc7c-5cbada44b481",
   "metadata": {},
   "source": [
    "## 3. 计算每个词的 embedding\n",
    "\n",
    "现在，我们可以计算每个词的 embedding 了。\n",
    "\n",
    "并且把 分词 和 embedding 存成如下形式：\n",
    "\n",
    "```python\n",
    "[\n",
    "    ['甄士隐'， [-0.9199, ... , -0.2456,  0.0457]],\n",
    "    ['梦幻'， [-0.7423, ... , -0.3301, -0.1229]],\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41c8413e-26b6-4f9b-9821-8eff1f33e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = get_embeddings(corpus=n_corpus)\n",
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "425a1db5-6804-4277-a22a-741c00943fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2embeddings = [[k, v] for k, v in zip(n_corpus, [np.array(e) for e in word_embeddings])]\n",
    "len(corpus2embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d9856-e6a3-45eb-b758-93223eb9e88b",
   "metadata": {},
   "source": [
    "## 4. 计算我们关心词汇的近邻 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e089c21d-c82c-4afa-9def-cdbfe0a190e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_embedding(embedding, embeddings):\n",
    "    squared_distances = np.sum((embeddings - embedding) ** 2, axis=1)\n",
    "    nearest_idx = np.argmin(squared_distances)\n",
    "\n",
    "    return embeddings[nearest_idx], nearest_idx\n",
    "\n",
    "def nearest_word(word, c2e):\n",
    "    word_list = [e[0] for e in c2e]\n",
    "    if word not in word_list:\n",
    "        raise Exception(f'{word} not in word list.')\n",
    "\n",
    "    embedding = [v for k, v in c2e if k == word][0]\n",
    "    embeddings = [v for k, v in c2e if k != word]\n",
    "    _, idx = nearest_embedding(embedding, embeddings)\n",
    "\n",
    "    return word_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe10f3a0-4f41-4f74-a775-5f63aee08e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'孩子'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_word(word='黛玉',\n",
    "             c2e=corpus2embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12203e42-8f2b-44a6-a582-32255b01d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'湘云'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_word(word='凤姐',\n",
    "             c2e=corpus2embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab7655-cb47-412a-b869-9724556d27ba",
   "metadata": {},
   "source": [
    "## 5. 整合成一个类\n",
    "\n",
    "整合以上功能，写成一个类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30802917-f4eb-4017-bd00-e98c082e51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    def __init__(self, model_path, book_path, stopwords_path, min_freq):\n",
    "        self.model_path = model_path\n",
    "        self.book_path = book_path\n",
    "        self.stopwords_path = stopwords_path\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        # 加载模型\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        self.model = BertModel.from_pretrained(model_path)\n",
    "\n",
    "    def cut(self):\n",
    "        \"\"\"分词\"\"\"\n",
    "        corpus = preprocess(self.book_path,\n",
    "                            self.stopwords_path)\n",
    "        ctr = collections.Counter(corpus)\n",
    "        return [k for k, v in ctr.items() if v > self.min_freq]\n",
    "\n",
    "    def get_embeddings(self, corpus):\n",
    "        \"\"\"批量计算 embeddings\"\"\"\n",
    "        encoded_inputs = self.tokenizer(corpus,\n",
    "                                        padding='max_length',\n",
    "                                        truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded_inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        return embeddings\n",
    "    \n",
    "    @staticmethod\n",
    "    def corpus2embeddings(corpus, embeddings):\n",
    "        \"\"\"存储 词汇 与 embedding 的映射关系\"\"\"\n",
    "        np_embeddings = [np.array(e) for e in embeddings]\n",
    "        return [[k, v] for k, v in zip(corpus, np_embeddings)]\n",
    "\n",
    "    @staticmethod\n",
    "    def nearest_embedding(embedding, embeddings):\n",
    "        \"\"\"计算近邻 embedding\"\"\"\n",
    "        squared_distances = np.sum((embeddings - embedding) ** 2, axis=1)\n",
    "        nearest_idx = np.argmin(squared_distances)\n",
    "\n",
    "        return embeddings[nearest_idx], nearest_idx\n",
    "\n",
    "    def nearest_word(self, word, c2e):\n",
    "        \"\"\"计算近邻 词汇\"\"\"\n",
    "        word_list = [e[0] for e in c2e]\n",
    "        if word not in word_list:\n",
    "            raise Exception(f'{word} not in word list.')\n",
    "\n",
    "        embedding = [v for k, v in c2e if k == word][0]\n",
    "        embeddings = [v for k, v in c2e if k != word]\n",
    "        _, idx = self.nearest_embedding(embedding, embeddings)\n",
    "        \n",
    "        return word_list[idx]\n",
    "\n",
    "    def test(self):\n",
    "        corpus = self.cut()\n",
    "        embeddings = self.get_embeddings(corpus)\n",
    "        c2e = self.corpus2embeddings(corpus, embeddings)\n",
    "\n",
    "        word = '宝玉'\n",
    "        nearest_word = self.nearest_word(word, c2e)\n",
    "        print('nearest_word:', nearest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2c8f465-f62d-4971-bbb6-4393f0e0b9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./data/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearest_word: 林之孝\n"
     ]
    }
   ],
   "source": [
    "c = Corpus(model_path=CN_BERT_PATH,\n",
    "           book_path=CN_BOOK_PATH,\n",
    "           stopwords_path=CN_STOP_WORDS,\n",
    "           min_freq=MIN_FREQ)\n",
    "c.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8daf391-54b2-41b4-ba73-b4b16c186066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
